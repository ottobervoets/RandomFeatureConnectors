
    # def load_patterns(self, patterns:list):
    #     for pattern in patterns:
    #         self.c[self.number_of_patterns_stored] = np.ones(self.N) #start at 1 conceptor
    #         self.pattern_length[self.number_of_patterns_stored] = len(pattern)
    #         # conceptor weight adaptation
    #         z = np.transpose(self.F) @ self.r_initial.copy() #right??
    #         for t in range(len(pattern)):
    #             z = self.c[self.number_of_patterns_stored] @ np.transpose(self.F) @ np.tanh(self.W @ self.F @ z +
    #                                                                                         self.W_in * pattern[t] +
    #                                                                                        self.b)
    #             if t > self.washout:
    #                 self.c[self.number_of_patterns_stored] = self.c[self.number_of_patterns_stored] + self.lr_c*(
    #                     z**2 - self.c[self.number_of_patterns_stored] * z**2 - self.aperture**-2 * self.c[self.number_of_patterns_stored]
    #                 )
    #
    #         r_collected = []
    #         z_collected = []
    #         z = np.transpose(self.F) @ self.r_initial.copy() #right??
    #         r = self.r_initial.copy()
    #         for t in range(len(pattern)):
    #             r = np.tanh(self.W @ self.F @ z + self.W_in * pattern[t] + t)
    #             z = self.c[self.number_of_patterns_stored] @ np.transpose(self.F) @ r
    #
    #             if t > self.washout:
    #                 r_collected.append(r)
    #                 z_collected.append(z)
    #
    #         # scipy.optimize.minimize()
    #
    #
    #
    #
    #
    #
    #         self.number_of_patterns_stored += 1

    # def objective_D(self, D_flat):
    #     D = D_flat.reshape(self.N, self.M)  # Reshape the flattened D into matrix form
    #     total_loss = 0
    #     for j in range(num_samples):
    #         for n in range(1, P.shape[1]):
    #             p_jn = P[j, n]
    #             z_jn = Z[j, n - 1]
    #             # Compute the loss for this sample and timestep
    #             loss = np.linalg.norm(W_in * p_jn - D @ z_jn) ** 2
    #             total_loss += loss
    #     # Add the regularization term
    #     total_loss += beta ** 2 * np.linalg.norm(D) ** 2
    #     return total_loss


rfc = RFCNetwork(10, 100, 10)
rfc.r[0] = 1
print(rfc.r_initial[0])


    def compute_W_out(self, r_recordings, patterns, beta_W_out):
        res = scipy.optimize.minimize(self.__W_out_objective, x0=self.W_out.flatten(),
                                      args=(r_recordings, patterns, beta_W_out))
        print(f"W objective = {self.__W_out_objective(self.D.flatten(), r_recordings, patterns, beta_W_out)}")
        return res.x.reshape(self.N, self.M)

    def __W_out_objective(self, W_flat, r_recordings, patterns, beta_W_out) -> float:
        W = W_flat.reshape(self.N, self.M)  # Reshape the flattened D into matrix form
        total_loss = 0
        for pattern, r_recording in zip(patterns, r_recordings):
            for pattern_step, r in zip(pattern, r_recording):
                total_loss += np.linalg.norm(pattern_step - W @ np.atleast_1d(r), 2)
        # Add the regularization term
        total_loss += beta_W_out ** 2 * np.linalg.norm(W, 2)
        return total_loss

            def compute_D(self, z_recordings, patterns, beta_D):
        res = scipy.optimize.minimize(self.__D_objective, x0=self.D.flatten(), args=(z_recordings, patterns, beta_D))
        print(f"D objective = {self.__D_objective(self.D.flatten(), z_recordings, patterns, beta_D)}")
        return res.x.reshape(self.N, self.M)

    def __D_objective(self, D_flat, z_recordings, patterns, beta_D) -> float:

        D = D_flat.reshape(self.N, self.M)  # Reshape the flattened D into matrix form
        total_loss = 0
        for pattern, z_recording in zip(patterns, z_recordings):
            for pattern_step, z in zip(pattern, z_recording):
                total_loss += np.linalg.norm(self.W_in * pattern_step - D @ z, 2)
        # Add the regularization term
        total_loss += beta_D ** 2 * np.linalg.norm(D, 2)
        return total_loss


        # print(np.repeat(self.W_in, self.number_of_patterns_stored).reshape(self.N * self.number_of_patterns_stored, 1).shape)
        # print(np.shape(np.hstack(p_recordings)))
        # y = np.repeat(self.W_in, self.number_of_patterns_stored).reshape(self.N * self.number_of_patterns_stored, 1) @ np.hstack(p_recordi
        print(np.shape(np.array(p_recordings).flatten()))
        y = np.repeat(self.W_in, len(p_recordings) * self.N)
        X = np.hstack(z_recordings)

        print(np.shape(y))
        print(np.shape(X))

        # print(np.shape(R @ np.transpose(R) - beta_W_out * np.diag(np.ones(self.N))))
        W_out_2 = (np.invert(R @ np.transpose(R) - beta_W_out * np.diag(np.ones(self.N))) @ R @ np.transpose(Q))




